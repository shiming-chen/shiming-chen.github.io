
<!DOCTYPE html>
<!-- saved from url=(0040)http://artemsheludko.pw/flexible-jekyll/ -->
<html lang="en">

<head>
	
<!-- <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?1833684faf5f254c1bb31386c5780c57";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script> -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-87320911-1', 'auto');
  ga('send', 'pageview');

</script>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Shiming Chen-陈使明</title>
<link rel="shortcut icon" href="img/chen1.jpg"/>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Shimingchen, EIC, HUST, The Huazhong University of Science and Technology"> 
<meta name="description" content="Shiming Chen's home page">
<meta name="google-site-verification" content="yy_3iiS_X6pJdegdwitJMrH0LRLHXwpjrV9RKLXxKjg" />
<meta name="google-site-verification" content="yy_3iiS_X6pJdegdwitJMrH0LRLHXwpjrV9RKLXxKjg" />
<!-- <link rel="stylesheet" href="./css/jemdoc.css"> -->
<link rel="stylesheet" href="./css/jemdoc.css" type="text/css">
<title>Shiming Chen, Huazhong University of Science and Technology</title>
</head>

<body>
 <div id="layout-content" style="margin-top:25px">
 <a href="https://github.com/shiming-chen" class="github-corner"><svg width="80" height="80" viewBox="0 0 250 250"
 style="fill:#0000FF; color:#fff; position: absolute; top: 0; border: 0; right: 0;"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z">
 </path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,
 87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
 <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 
 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,
 77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,
 116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>
 <style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,
 60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px)
 {.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
	
	
	
	
<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">					
					<h1>Shiming Chen &nbsp 陈使明 </h1><h1>
				</h1></div>

<!-- 				<h3> Postdoctoral Research Fellow </h3> -->
				<p>
<!-- 					1037 Luoyu Road,<br>
					National Anti-counterfeit Engineering Research Center,<br>
					Huazhong University of Science and Technology (HUST), <br>
					Wuhan, China, 430074 <br>
					<br> -->
					Email:  gchenshiming <strong>at</strong> gmail <strong>dot</strong> com
					       
				</p>
				<p>     
					
					<!-- <a href="paper/CV.pdf"><img src="img/cv_p.jpg"  height="40px" style="margin-bottom:-3px"></a> -->
					<a href="https://github.com/shiming-chen"><img src="img/github.jpg" height="40px" style="margin-bottom:-3px"></a>
					<a href="https://scholar.google.com/citations?hl=en&user=botczdcAAAAJ"><img src="img/google-logo.png"  height="40px" style="margin-bottom:-3px"></a>
<!-- 					<a href="img/weichat.pdf"><img src="img/weichat.jpg"  height="40px" style="margin-bottom:-3px"></a> -->
				</p>
			</td>
			<td>
				<a href="https://shiming-chen.github.io/"><img src="img/chen1.jpg" alt="Shiming Chen" border="0" width="300"></a><br>
			</td>
		</tr><tr>
	</tr>
	</tbody>
</table>
	
    <h2>Short Bios</h2>
    <div id="news-content" >
<!-- 	  <span>I come from Meizhou, Guangdong, where is known as “<i>Hakka capital</i>” and “<i> City of Football </i>”.<br> -->
	  <span>
<!-- 		I am currently a Postdoctoral Research Fellow in the <a href="https://www.cmu.edu/dietrich/causality/">Causal Learning and Reasoning (CLeaR)</a> 
		research group at CMU and the <a href="https://mbzuai.ac.ae/research/research-center/ciai/">Center for Integrative AI (CIAI)</a> at MBZUAI, 
		fortunately working with Prof.  <a href="https://www.andrew.cmu.edu/user/kunz1/index.html">Kun Zhang</a>. 
		Prior to that,  -->
		I received my Ph.D. degree at Huazhong University of Science and Technology in Dec. 2022, advised by Prof. <a href="http://bmal.hust.edu.cn/info/1005/1091.html">Xinge You</a> and worked closely Prof. <a href="https://scholar.google.com/citations?user=z84rLjoAAAAJ">Ling Shao</a>.
                My current research interests span computer vision and machine learning with a series of topics, such as <strong><i>zero-shot learning</i></strong>, <strong><i>generative modeling and learning</i></strong>, and <strong><i>visual-and-language learning</i></strong>. </span> <br> <br> 
<!-- 	       <span>I am the <a href="http://valser.org/article-634-1.html">AC</a> of <strong>VALSE</strong>.</span><br> -->
<!-- 	  <span style="color:Red"><strong>Open Position:</strong> If you are interested in our researches (open-world learning with limited data, e.g., zero/few-shot learning, OOD, OSR) and 
	    would like to work with Prof. <a href="https://scholar.google.com/citations?user=zvaeYnUAAAAJ&hl=zh-CN">Fahad Shahbaz Khan</a>, Prof. <a href="https://salman-h-khan.github.io/">Salman Khan</a>  and me as an intern/visiting student in the MBZUAI,
	    feel free drop me email with your CV. </span> -->
    </div>
	 

      <h2 >News</h2>
<!--     <div style="margin-top: 15px; overflow-y: scroll;"> -->
      <div style="height: 320px; overflow: auto;">
      <ul>
      <li><span style="color:Red">2025.02</span>, Our paper on zero-shot learning is accepted to <strong><i>IJCV</i></strong>.</li>
      <li><span style="color:Red">2025.02</span>, Our paper on zero-shot learning is accepted to <strong><i>ICLR'25</i></strong>, congrating to Zihan.</li>
      <li><span style="color:Red">2024.12</span>, Our TWO papers on zero/few-shot learning is accepted to <strong><i>AAAI'25</i></strong>, congrating to Wenjin and Yunwei.</li>
      <li><span style="color:Red">2024.11</span>, I received <strong>2024 CSIG Excellent Doctoral Dissertation Award Honorable Mention Award </strong>.</li>
      <li><span style="color:Red">2024.11</span>, Our paper on zero-shot learning is accepted to <strong><i>IEEE TNNLS</i></strong>, congrating to Bowen.</li>
      <li><span style="color:Red">2024.09</span>, Our paper on audio-visual zeroshot learning is accepted to <strong><i>IEEE TETCI</i></strong>, congrating to Yujie.</li>
       <li><span style="color:Red">2024.08</span>, Our paper on domain generalization is accepted to <strong><i>IEEE TMM</i></strong>, congrating to Guanglin.</li>
      <li><span style="color:Red">2024.07</span>, Our TWO papers on zero/few-shot learning is accepted to <strong><i>ACM MM'24</i></strong>, congrating to Shuhuang and Yunwei.</li>
      <li><span style="color:Red">2024.05</span>, Our paper on few-shot object detection is accepted to <strong><i>IEEE TIP</i></strong>, congrating to Zhimeng.</li>
      <li><span style="color:Red">2024.02</span>, Our TWO paper on zero-shot learning are accepted to <strong><i>CVPR'24</i></strong>.</li>
      <li><span style="color:Red">2024.02</span>, Our SURVEY on few-shot object detection is accepted to <strong><i>Information Fusion</i></strong>, congrating to Zhimeng.</li>
      <li><span style="color:Red">2024.01</span>, Our paper on Non-Transferable Representation Learning is accepted to <strong><i>ICLR'24</i></strong> (<span style="color:Red"><strong>Spotlight</strong></span>), congrating to Ziming.</li>
      <li><span style="color:Red">2023.12</span>, Our paper on zero-shot learning is accepted to <strong><i>Science China Information Sciences</i></strong>, congrating to Shuhuang.</li>
      <li><span style="color:Red">2023.11</span>, I give a talk in Harbin Institute of Technology (Shenzhen), invited by <strong><a href="https://www.dl2link.com/">Prof. Haijun Zhang</a></strong>.</li>
      <li><span style="color:Red">2023.10</span>, I organize a <strong>VALSE Webinar</strong> on <a href="https://mp.weixin.qq.com/s/HzrGKmq4jXam7DCFNIMMFg"> "Zero-shot Learning in Vision"</a>. 
	      Thanks for the exciting sharing of <strong><a href="https://faculty.uestc.edu.cn/jjl">Prof. Jingjing Li</a></strong>,
	      <strong><a href="https://kaiyangzhou.github.io/">Prof. Kaiyang Zhou</a></strong>,
	      <strong><a href="https://xianyongqin.github.io">Dr. Yongqin Xian</a></strong>, and <strong><a href="http://seea.tju.edu.cn/info/1014/1447.htm">Prof. Zhong Ji</a></strong>.
      </li>
      <li><span style="color:Red">2023.09</span>, I give a talk in ZHEJIANG LAB, invited by <strong>Dr. Jin Zhao</strong>.</li>
      <li><span style="color:Red">2023.08</span>, one paper on zero-shot learning is accepted to <strong><i>TEC</i></strong>.</li>
      <li><span style="color:Red">2023.07</span>, Our paper on rainy image generation is accepted to <strong><i>ICCV'23</i></strong>.</li>
      <li><span style="color:Red">2023.07</span>, I am invited as an <strong><i>Area Chair (AC)</i></strong> of PRCV'23.</li>
      <li><span style="color:Red">2023.06</span>, I give a talk in University of Science and Technology of China, invited by <strong><a href="http://imcc.ustc.edu.cn/_upload/tpl/0d/13/3347/template3347/xiehongtao.html">Prof. Hongtao Xie</a></strong>.</li>
      <li><span style="color:Red">2023.04</span>, Our paper on zero-shot learning is accepted to <strong><i>ICML'23</i></strong>.</li>
      <li><span style="color:Red">2023.04</span>, I give a talk in Alibaba DAMO Academic, invited by <strong>Baigui Sun</strong>.</li>
      <li><span style="color:Red">2023.04</span>, I give a talk in Huazhong Agricultural Univeristy, invited by <strong><a href="https://chenhongml.github.io/">Prof. Hong Chen</a></strong>.</li>
      <li><span style="color:Red">2023.04</span>, I give a talk in Guizhou University, invited by <strong><a href="https://scholar.google.com/citations?hl=en&user=7879e5QAAAAJ">Prof. Yisong Wang</a></strong>.</li>
      <li><span style="color:Red">2023.03</span>, I am invited as an <strong><i>Area Chair (AC)</i></strong> of VALSE, news at <strong><a href="http://valser.org/article-634-1.html">Here</a></strong>.</li>
      <li><span style="color:Red">2023.03</span>, I give a talk in National Key Laboratory of Science and Technology on Multispectral Information Processing, invited by <strong><a href="https://owuchangyuo.github.io/">Prof. Yi Chang</a></strong>.</li>
      <li><span style="color:Red">2022.12</span>, Our TransZero++ is accepted to <strong><i>TPAMI</i></strong>.</li>
      <li><span style="color:Red">2022.08</span>, I was invited as a Program Committee (PC) Member for <strong><i>AAAI'23</i></strong>.</li>
      <li><span style="color:Red">2022.05</span>, I give a talk about our CVPR'22 work (MSDN) in VALSE.</li>
      <li><span style="color:Red">2022.05</span>, I give a talk about zero-shot learning in AI TIME and AI Drive.</li>
      <li><span style="color:Red">2022.04</span>, Our paper on zero-shot learning is accepted to <strong><i>IJCAI'22</i></strong>.</li>
      <li><span style="color:Red">2022.04</span>, We have released the full codes of <strong><a href="https://arxiv.org/pdf/2112.01683.pdf">TransZero</a></strong> accepted to AAAI'22.</li>
      <li><span style="color:Red">2022.03</span>, Our paper on zero-shot learning is accepted to <strong><i>CVPR'22</i></strong>.</li>
      <li><span style="color:Red">2022.02</span>, I gave a talk in Extreme Mart (极市).</li>
      <li><span style="color:Red">2022.02</span>, Our paper on zero-shot learning is accepted to <strong><i>TNNLS</i></strong>.</li>
      <li><span style="color:Red">2022.02</span>, I gave a talk in AI TIME PhD-NeurIPS, invite by AI TIME.</li>
      <li><span style="color:Red">2022.01</span>, I start a Research Intern at <strong>Tencent AI Lab</strong>.</li>
      <li><span style="color:Red">2021.12</span>, Our paper on zero-shot learning is accepted to <strong><i>AAAI'22</i></strong>.</li>
      <li><span style="color:Red">2021.09</span>, Our paper on zero-shot learning is accepted to <strong><i>NeurIPS'21</i></strong>.</li>
      <li><span style="color:Red">2021.07</span>, Our paper on zero-shot learning is accepted to <strong><i>ICCV'21</i></strong>.</li>
      <!--<li ><span style="color:Red">2021.05</span>, I start a Research Intern at <strong>Alibaba DAMO Academy</strong>.</li>
      <li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2021.04.29</span>, one co-authored paper is accepted to <strong><i>IJCAI'21</i></strong>.</li>
      <li><span style="color:Red">2021.02</span>, Our paper is accepted to <strong><i>IEEE Transactions on Evolutionary Computation (TEC)</i></strong>.</li>
      <li><span style="color:Red">2019.09</span>, I start my Ph.D study in EIC at <strong>HUST</strong>.</li>
      <li><span style="color:Red">2019.01</span>, Our paper is accepted to <strong><i>Information Sciences (INS)</i></strong>.</li>
      <li><span style="color:Red">2018.10</span>, Our paper is accepted to <strong><i>ACTA AUTOMATICA SINICA</i></strong>. </li> -->
      </ul>
    </div>
     
 


     <h2>Researches</h2>
    <div id="news-content" >
	   <span>
         A key challenge of artificial intelligence is to generalize machine learning models from seen data to unseen scenarios.
	<strong><i>Zero-shot learning (ZSL)</i></strong> is a typical research topic targeting this goal. ZSL aims to classify the images of unseen classes by constructing a mapping relationship between the semantic and visual domains.
	Although ZSL has achieved significant progress, there have a numbers of essential challenges. Recently, large-scale VLM-based ZSL method is popular, e.g., CLIP, it is an extension of the classical ZSL. <br><br>
		   
         Dr. Shiming Chen has been focusing on tackling bottleneck challenges to promote ZSL (especially for the classical ZSL), covering fundamental questions of
         <i>How to enhance the visual features by alleviating the cross-dataset bias between pre-train dataset and ZSL benchmarks? 
	 How to discover the intrinsic semantic knowledge by alleviating the visual-semantic domain shift problem? 
	How to align the visual and semantic features in a common space by reducing the discrepancy between the heterogeneous visual-semantic representations?</i>
	Specifically, his three representatives research projects are: <br> <br> 
	
         <strong>1. Developing the visual feature enhancement algorithms to tackle the challenge of cross-dataset bias in ZSL. </strong> 
         As for the embedding-based ZSL, a graph-guided dual attention network is introduced to fuse the local visual features and explicit global visual features
	to enhance visual features. As for the generative ZSL, several feature refinement learning methods are proposed to enhance the visual features and encourage
	the generator to synthesize realistic visual features for unseen classes. The papers of this project have been published in 
	<i> <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Chen_FREE_Feature_Refinement_for_Generalized_Zero-Shot_Learning_ICCV_2021_paper.html" target="_blank"> ICCV'21</a>, 
	<a href="https://www.ijcai.org/proceedings/2022/134" target="_blank"> IJCAI'22</a>, 
	<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9768177" target="_blank"> IEEE TNNLS'22</a>,  
	 etc.<br> <br> 
		   
        <strong>2. Developing the effective ZSL algorithms to tackle the visual-semantic domain shift problem. </strong> 
	As for the embedding-based ZSL, a attribute-guided Transformer network and mutually semantic distillation network are proposed to learn the intrinsic 
	semantic knowledge, enriching the visual features with semantic information to enable desirable semantic knowledge transfer from seen calsses to unseen ones.
	As for the generative ZSL, dynamic semantic prototype learning is proposed to refine the pre-defined semantic prototypes under the guidance of visual signal,
	 aligning the empirical and actual semantic prototypes for synthesizing accurate visual features. The papers of this project have been published in 
	<i> <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chen_MSDN_Mutually_Semantic_Distillation_Network_for_Zero-Shot_Learning_CVPR_2022_paper.html" target="_blank"> CVPR'22</a>, 
		<a href="https://ojs.aaai.org/index.php/AAAI/article/view/19909" target="_blank"> AAAI'22</a>, 
		<a href="https://ieeexplore.ieee.org/document/9987664" target="_blank"> IEEE TPAMI'22</a>, 
	        <a href="https://ieeexplore.ieee.org/document/10225587" target="_blank"> IEEE TEC'23</a>,
		<a href="https://proceedings.mlr.press/v202/chen23l/chen23l.pdf" target="_blank"> ICML'23</a> ,
		<a href="https://arxiv.org/pdf/2404.14808">CVPR'24</a>
	       </i>, etc.<br> <br> 
		   
       <strong>3. Developing the semantic-visual adaptation framework for visual-semantic alignment.</strong> 
	Different to existing one-step adaptation method that on alignment the feature distributions between visual and semantic domains, 
	this method utilizes a hierarchical adaptation to learn an intrinsic common space for semantic and visual feature representations
	by adopting sequential structure adaptation and distribution adaptation. The papers of this project have been published in 
	<i>
		<a href="https://proceedings.neurips.cc/paper/2021/hash/8b0d268963dd0cfb808aac48a549829f-Abstract.html" target="_blank"> NeurIPS'21</a>,
		<a href="https://arxiv.org/abs/2404.07713">CVPR'24</a>
	</i>.
	</span> <br> <br> 
	<span></span><br>
<!-- 	  <span>I come from Meizhou, Guangdong, where is known as “<i>Hakka capital</i>” and “<i> City of Football </i>”.<br> -->
<!-- 	  <span>I currently a Postdoctoral Research Fellow in the <a href="https://www.cmu.edu/dietrich/causality/">Causal Learning and Reasoning (CLeaR)</a> research group at CMU and the <a href="https://mbzuai.ac.ae/research/research-center/ciai/">Center for Integrative AI (CIAI)</a> at MBZUAI, working with Prof.  <a href="https://www.andrew.cmu.edu/user/kunz1/index.html">Kun Zhang</a>. Prior to that, I received my Ph.D. degree at Huazhong University of Science and Technology in 2022, advised by Prof. <a href="http://bmal.hust.edu.cn/info/1005/1091.htm">Xinge You</a>.
	  I was also visiting at <a href="https://www.tmllab.ai/">Trustworthy Machine Learning Lab (TML Lab)</a>, University of Sydney, working with Prof. <a href="https://tongliang-liu.github.io/index.html">Tongliang Liu</a>.
          My current research interests span computer vision and machine learning with a series of topics, such as <strong><i>zero-shot learning</i></strong>, <strong><i>generative modeling and learning</i></strong>, and <strong><i>visual-and-language learning</i></strong>. I am the <a href="http://valser.org/article-634-1.html">AC</a> of VALSE.</span> <br> <br> 
	  <span></span><br> -->
<!-- 	  <span style="color:Red">I'm Looking forward to a position for work or postdoc researcher, please feel free to drop me an email if you have the relevant position.</span> -->
    </div>
	  
     
<tr><tr><tr><tr>
<div style="margin-top: 10px"></div>
    <h2>Latest Publications <a href="https://scholar.google.com/citations?hl=en&user=botczdcAAAAJ">(Full List)</a> </h2>
<!--<p><a href="https://scholar.google.com/citations?hl=en&user=botczdcAAAAJ">My Google Scholar</a></p>-->
	 
<!--<table id="tbPublications" width="100%">
	<tbody>
<h3 style="color: red">Preprints</h3>
		
   <tr>	
		<td width="206">
		<img src="img/TransZero-TPAMI.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td><a href="TransZero-pp/TransZero-pp.html" target="_blank">TransZero++: Cross Attribute-guided Transformer for Zero-Shot Learning.</a><br>
		<p ><strong>Shiming Chen</strong>, Ziming Hong, Guo-Sen Xie, Jian Zhao, Xinge You, Shuicheng Yan, Ling Shao.</p>
        <p class="post-date" style="margin-top: -10px" ><i>arXiv preprint arXiv: 2111.04254</i>, 2021. <strong>
		[<a href="TransZero-pp/TransZero-pp.html">Project Page</a>][<a href="http://arxiv.org/abs/2112.08643">arXiv</a>]
		[<a href="https://github.com/shiming-chen/TransZero_pp">Code</a>]</strong>
		</p>		
	 <p style="margin-top: -11px"><span style="color:Green">Submitted to <i>IEEE Transactions Pattern Analysis and Machine Intelligence (<strong> TPAMI </strong>)</i> (Minor Revision)</span></p>
		</td>
	</tr>
	<tr></tr>
    <tr></tr>
    
 </tbody>
</table>-->	      
<table id="tbPublications" width="100%">
<tbody>
<span>(<strong>*:Co-First Author; #:Corresponding Author</strong>) </span>
<!--<h3 style="color: red">Survey Papers </h3>

 <tr>
		<td width="206">
		<img src="img/2024-TKDE-causality-survey.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="" target="_blank">Emerging Synergies in Causality and Deep Generative Models: A Survey.</a>
			<strong>[<a href="https://arxiv.org/pdf/2301.12351.pdf">arXiv</a>]</strong>
		 <p >Guanglin Zhou, Shaoan Xie, Guangyuan Hao, <strong>Shiming Chen</strong>, Biwei Huang, Xiwei Xu, Chen Wang, Liming Zhu, Lina Yao, Kun Zhang.</p>
       <p style="margin-top: -11px"><i>arXiv preprint arxiv:2301.12351, 2023. </i></p>
		</td>
	</tr>
	<tr></tr>
	<tr></tr> 

	
<tr>
		<td width="206">
		<img src="img/FSOD-survey.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="https://www.sciencedirect.com/science/article/pii/S156625352400085X" target="_blank"> Few-shot object detection: Research advances and challenges.</a>
			<strong>[<a href="https://www.sciencedirect.com/science/article/pii/S156625352400085X">PDF</a>]
				[<a href="https://arxiv.org/pdf/2404.04799.pdf">arXiv</a>]</strong>
		 <p >Zhimeng Xin, <strong>Shiming Chen</strong>, Tianxu Wu, Yuanjie Shao, Weiping Ding, Xinge You.</p>
       <p style="margin-top: -11px"><i>Information Fusion, 108, 102307 (2024). <strong>(<span style="color:Burlywood">SCI, IF=18.6</span>)</strong> </i></p>
		</td>
	</tr>
	<tr></tr>
	<tr></tr>
</tbody>
</table>-->
	

<table id="tbPublications" width="100%">
<tbody>	
<h3 style="color: red">Conference Papers </h3>	

	
<tr>
	
		<td width="206">
		<img src="img/cvpr24-zslvit.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="" target="_blank"> Progressive Semantic-Guided Vision Transformer for  Zero-Shot Learning.</a>
			<strong>[<a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_Progressive_Semantic-Guided_Vision_Transformer_for_Zero-Shot_Learning_CVPR_2024_paper.html">PDF</a>]
				[<a href="https://arxiv.org/abs/2404.07713">arXiv</a>]
				[<a href="https://github.com/shiming-chen/ZSLViT">Code</a>]
			</strong>
		 <p ><strong>Shiming Chen</strong>, Wenjin Hou, Salman Khan, Fahad Shahbaz Khan.</p>
       <p style="margin-top: -11px"><i>IEEE Conference on Computer Vision and Pattern Recognition (<strong> CVPR </strong>), 2024. <strong>(<span style="color:Burlywood">CCF Rank-A</span>)</strong> </i></p>
		</td>
	</tr>
	<tr></tr>
	<tr></tr>

	
<tr>
	
		<td width="206">
		<img src="img/ICML23-DSP.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="https://proceedings.mlr.press/v202/chen23l.html" target="_blank"> Evolving Semantic Prototype Improves Generative Zero-Shot Learning.</a>
			<strong>[<a href="https://proceedings.mlr.press/v202/chen23l/chen23l.pdf">PDF</a>]
				[<a href="https://arxiv.org/pdf/2306.06931.pdf">arXiv</a>]
			</strong>
		 <p ><strong>Shiming Chen</strong>, Wenjin Hou, Ziming Hong, Xiaohan Ding, Yibing Song, Xinge You, Tongliang Liu, Kun Zhang.</p>
       <p style="margin-top: -11px"><i>The Fortieth International Conference on Machine Learning (<strong> ICML </strong>), 2023. <strong>(<span style="color:Burlywood">CCF Rank-A</span>)</strong> </i></p>
		</td>
	</tr>
	<tr></tr>
	<tr></tr>	
	


<tr>
	
		<td width="206">
		<img src="img/CVPR22.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chen_MSDN_Mutually_Semantic_Distillation_Network_for_Zero-Shot_Learning_CVPR_2022_paper.html" target="_blank"> MSDN: Mutually Semantic Distillation Network for Zero-Shot Learning.</a>
			<strong>[<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_MSDN_Mutually_Semantic_Distillation_Network_for_Zero-Shot_Learning_CVPR_2022_paper.pdf">PDF</a>]
				[<a href="https://arxiv.org/abs/2203.03137">arXiv</a>]
				[<a href="https://github.com/shiming-chen/MSDN">Code</a>]</strong><br>
		 <p ><strong>Shiming Chen</strong>, Ziming Hong,  Guo-Sen Xie, Wenhan Yang, Qinmu Peng, Kai Wang, Jian Zhao, Xinge You.</p>
       <p style="margin-top: -11px"><i>IEEE Conference on Computer Vision and Pattern Recognition (<strong> CVPR </strong>), 2022: 7612-7621. <strong>(<span style="color:Burlywood">CCF Rank-A</span>)</strong> </i></p>
		</td>
	</tr>
	<tr></tr>
    <tr></tr>	


<tr>
	
		<td width="206">
		<img src="img/TransZero-AAAI22.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/19909" target="_blank"> TransZero: Attribute-guided Transformer for Zero-Shot Learning.</a>
			<strong>[<a href="https://ojs.aaai.org/index.php/AAAI/article/view/19909">PDF</a>]</strong>
			<strong>[<a href="https://arxiv.org/pdf/2112.01683.pdf">arXiv</a>]</strong>
			<strong>[<a href="https://github.com/shiming-chen/TransZero">Code</a>]</strong><br>
		 <p ><strong>Shiming Chen<sup>*</sup></strong>, Ziming Hong<sup>*</sup>, Yang Liu, Guo-Sen Xie, Baigui Sun, Hao Li, Qinmu Peng, Ke Lu, Xinge You.</p>
       <p style="margin-top: -11px"><i>Thirty-Sixth AAAI Conference on Artificial Intelligence (<strong> AAAI </strong>), 2022: 330-338. <strong>(<span style="color:Burlywood">CCF Rank-A</span>)</strong> </i></p>
		</td>
	</tr>
	<tr></tr>
    <tr></tr>

<tr>
	
		<td width="206">
		<img src="img/motivation-new.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="https://proceedings.neurips.cc/paper/2021/hash/8b0d268963dd0cfb808aac48a549829f-Abstract.html" target="_blank"> HSVA: Hierarchical Semantic-Visual Adaptation for Zero-Shot Learning.</a>
			<strong>[<a href="https://proceedings.neurips.cc/paper/2021/file/8b0d268963dd0cfb808aac48a549829f-Paper.pdf">PDF</a>]
				[<a href="https://arxiv.org/abs/2109.15163">arXiv</a>]
				[<a href="https://github.com/shiming-chen/HSVA">Code</a>]</strong><br>
		 <p ><strong>Shiming Chen</strong>, Guo-Sen Xie, Qinmu Peng, Yang Liu, Baigui Sun, Hao Li, Xinge You, Ling Shao.</p>
       <p style="margin-top: -11px"><i>Annual Conference on Neural Information Processing Systems (<strong> NeurIPS </strong>), 2021: 16622-16634. <strong>(<span style="color:Burlywood">CCF Rank-A</span>)</strong> </i></p>
		</td>
	</tr>
	<tr></tr>
	<tr></tr>
	
<tr>
	
		<td width="206">
		<img src="img/FREE.jpg" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Chen_FREE_Feature_Refinement_for_Generalized_Zero-Shot_Learning_ICCV_2021_paper.html" target="_blank">
			FREE: Feature Refinement for Generalized Zero-shot Learning.</a>
			<strong>[<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_FREE_Feature_Refinement_for_Generalized_Zero-Shot_Learning_ICCV_2021_paper.pdf">PDF</a>]
				[<a href="https://arxiv.org/abs/2107.13807">arXiv</a>]</strong>
			<strong>[<a href="https://github.com/shiming-chen/FREE">Code</a>]</strong><br>
		 <p ><strong>Shiming Chen</strong>, Wenjie Wang, Beihao Xia, Qinmu Peng, Xinge You, Feng Zheng, Ling Shao.</p>
       <p style="margin-top: -11px"><i> IEEE International Conference on Computer Vision (<strong> ICCV </strong>), 2021: 1106-1112. <strong>(<span style="color:Burlywood">CCF Rank-A</span>)</strong> </i></p>
		</td>
	</tr>
	<tr></tr>
    <tr></tr>
    <tr></tr>	   


			
	
	
 
<tr>
	
		<td width="206">
		<img src="img/cvpr24-vads.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="" target="_blank"> Visual-Augmented Dynamic Semantic Prototype for Generative Zero-Shot Learning.</a>
			<strong>[<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Hou_Visual-Augmented_Dynamic_Semantic_Prototype_for_Generative_Zero-Shot_Learning_CVPR_2024_paper.pdf">PDF</a>]
				[<a href="https://arxiv.org/pdf/2404.14808">arXiv</a>]
				[<a href="https://github.com/Houwenjin/VADS">Code</a>]
			</strong>
		 <p >Wenjin Hou, <strong>Shiming Chen<sup>#</sup></strong>, Shuhuang Chen, Ziming Hong, Yan Wang, Xuetao Feng, Salman Khan, Fahad Shahbaz Khan, Xinge You.</p>
       <p style="margin-top: -11px"><i>IEEE Conference on Computer Vision and Pattern Recognition (<strong> CVPR </strong>), 2024. <strong>(<span style="color:Burlywood">CCF Rank-A</span>)</strong> </i></p>
		</td>
	</tr>
	<tr></tr>
	<tr></tr>
  
    <tr>	
		<td width="206">
		<img src="img/IJCAI22.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td><a href="https://www.ijcai.org/proceedings/2022/134" target="_blank">Semantic Compression Embedding for Generative Zero-Shot Learning.</a>
			<strong>[<a href="https://www.ijcai.org/proceedings/2022/0134.pdf">PDF</a>]
				[<a href="https://github.com/HHHZM/SC-EGG">Code</a>]
			</strong>
		<p >Ziming Hong*, <strong>Shiming Chen<sup>*#</sup></strong>, Guo-Sen Xie, Wenhan Yang, Jian Zhao, Yuanjie Shao, Qinmu Peng, Xinge You</p>
	       <p style="margin-top: -11px"><i>The 31th International Joint Conference on Artificial Intelligence (<strong> IJCAI </strong>), 2022: 956-963. <strong>(<span style="color:Burlywood">CCF Rank-A</span>)</strong>
		</td>
	</tr>
	<tr></tr>
    <tr></tr>
    <tr></tr>
<!-- <tr>
	
		<td width="206">
		<img src="img/mm24-zsl.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="" target="_blank"> Causal Visual-semantic Correlation for Zero-shot Learning.</a>
			<strong>[<a href="https://arxiv.org/abs/2404.07713">arXiv</a>]</strong>
		 <p >Shuhuang Chen, Dingjie Fu, <strong>Shiming Chen</strong>, Shuo Ye, Wenjin Hou, Xinge You.</p>
       <p style="margin-top: -11px"><i>ACM International Conference on Multimedia(<strong> ACM MM </strong>), 2024. <strong>(<span style="color:Burlywood">CCF Rank-A</span>)</strong> </i></p>
		</td>
	</tr>
	<tr></tr>
	<tr></tr>

<tr>
	
		<td width="206">
		<img src="img/mm24-fsl.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="" target="_blank"> FSL-QuickBoost: Minimal-Cost Ensemble for Few-Shot Learning.</a>
 			<strong>[<a href="https://arxiv.org/abs/2404.07713">arXiv</a>]</strong> 
		 <p >Yunwei Bai, Bill Cai, Ying Kiat Tan, Zangwei Zheng, <strong>Shiming Chen</strong>, Tsuhan Chen.</p>
       <p style="margin-top: -11px"><i>ACM International Conference on Multimedia(<strong> ACM MM </strong>), 2024. <strong>(<span style="color:Burlywood">CCF Rank-A</span>)</strong> </i></p>
		</td>
	</tr>
	<tr></tr>
	<tr></tr> -->
 
<!--    

		       
     <tr>	
		<td width="206">
		<img src="img/IJCAI21.jpg" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td><a href="https://www.ijcai.org/proceedings/2021/0153.pdf" target="_blank">Norm-guided Adaptive Visual Embedding for Zero-Shot Sketch-Based Image Retrieval.</a>
			<strong>[<a href="https://www.ijcai.org/proceedings/2021/0153.pdf">PDF</a>]</strong><br>
		<p >Wenjie Wang, Yufeng Shi, <strong>Shiming Chen</strong>, Qinmu Peng, Feng Zheng, Xinge You</p>
	       <p style="margin-top: -11px"><i>The 30th International Joint Conference on Artificial Intelligence (<strong> IJCAI </strong>), 2021. <strong>(<span style="color:Burlywood">CCF Rank-A</span>)</strong>
		</td>
	</tr>
	<tr></tr>
    <tr></tr>
    <tr></tr> -->
</tbody>
</table>
		
<table id="tbPublications" width="100%">
<tbody>	
<h3 style="color: red">Journal Papers</h3>


<tr>	
		<td width="206">
		<img src="img/IJCV24-VIFR.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td><a href="https://link.springer.com/article/10.1007/s11263-025-02394-7" target="_blank">Semantics-Conditioned Generative Zero-Shot Learning via Feature Refinement.</a>
		 <strong>
		[<a href="https://link.springer.com/article/10.1007/s11263-025-02394-7">PDF</a>]
		[<a href="https://github.com/shiming-chen/ViFR">Code</a>]
		 </strong>
		<br>
		<p ><strong>Shiming Chen</strong>, Ziming Hong, Xinge You,Ling Shao.</p>
	 <p style="margin-top: -11px"> <i>International Journal of Computer Vision (<strong> IJCV </strong>), In press, 2025. <strong>
		 (<span style="color:Burlywood">SCI, IF=14.5, CCF Rank-A</span>)</strong></p>
		</td>
	</tr>
	<tr></tr>
<tr></tr>

	
<tr>	
		<td width="206">
		<img src="img/TransZero-TPAMI.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td><a href="https://ieeexplore.ieee.org/document/9987664" target="_blank">TransZero++: Cross Attribute-guided Transformer for Zero-Shot Learning.</a>
		 <strong>
		[<a href="TransZero-pp/TransZero-pp.html">Project Page</a>]
		[<a href="https://ieeexplore.ieee.org/document/9987664">PDF</a>]
		[<a href="http://arxiv.org/abs/2112.08643">arXiv</a>]
		[<a href="https://github.com/shiming-chen/TransZero_pp">Code</a>]
		 </strong>
		<br>
		<p ><strong>Shiming Chen</strong>, Ziming Hong, Wenjin Hou, Guo-Sen Xie,  Yibing Song, Jian Zhao, Xinge You, Shuicheng Yan, Ling Shao.</p>
	 <p style="margin-top: -11px"> <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong> TPAMI </strong>), 45(11):12844-12861, 2023. <strong>
		 (<span style="color:Burlywood">SCI, IF=24.314, CCF Rank-A</span>)</strong></p>
		</td>
	</tr>
	<tr></tr>
<tr></tr>


	
<tr>	
		<td width="206">
		<img src="img/EGANS-TEVC23.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td><a href="https://ieeexplore.ieee.org/document/10225587" target="_blank">EGANS: Evolutionary Generative Adversarial Network Search for Zero-Shot Learning.</a>
		<strong>[<a href="https://ieeexplore.ieee.org/document/10225587">PDF</a>]
		 [<a href="https://arxiv.org/abs/2308.09915">arXiv</a>]
		[<a href="https://github.com/shiming-chen/EGANS">Code</a>]
		</strong>
		<p ><strong>Shiming Chen</strong>, Shuhuang Chen, Wenjin Hou,Weiping Ding, Xinge You.</p>
	       <p style="margin-top: -11px"><i>IEEE Transactions on Evolutionary Computation (<strong> TEC </strong>), 28(3):582-596, 2024. 
		       <strong>(<span style="color:Burlywood">SCI, IF=14.3, CCF Rank-B</span>)</strong></p>
		</td>
	</tr>
    <tr></tr>

	
	
    <tr>	
		<td width="206">
		<img src="img/GNDAN.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td><a href="https://ieeexplore.ieee.org/document/9768177" target="_blank">GNDAN: Graph Navigated Dual Attention Network for Zero-Shot Learning.</a>
			<strong>
				[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9768177">PDF</a>]
				[<a href="https://github.com/shiming-chen/GNDAN">Code</a>]
			</strong>
		<p ><strong>Shiming Chen</strong>, Ziming Hong, Guo-Sen Xie, Xinge You, Weiping Ding and Ling Shao.</p>
	       <p style="margin-top: -11px"><i>IEEE Transactions on Neural Networks and Learning Systems (<strong> TNNLS</strong>), 35(4):4516-4529, 2024. <strong>
		 (<span style="color:Burlywood">SCI, IF=14.255, CCF Rank-B</span>)</strong></p>
	<tr></tr>
    <tr></tr>
    <tr></tr>
    
		       
		      
    <tr>	
		<td width="206">
		<img src="CDE-GAN-website/figures/CDEGAN-pipeline.jpg" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td><a href="CDE-GAN-website/CDE-GAN.html" target="_blank">CDE-GAN: Cooperative Dual Evolution Based Generative Adversarial Network.</a>
		<strong>[<a href="https://ieeexplore.ieee.org/document/9386237">PDF</a>]
		 [<a href="https://arxiv.org/abs/2008.09388">arXiv</a>]
		[<a href="https://github.com/shiming-chen/CDE-GAN">Code</a>]
		</strong>
		<p ><strong>Shiming Chen</strong>, Wenjie Wang, Beihao Xia, Xinge You, Qinmu Peng, Zehong Cao, Weiping Ding.</p>
	       <p style="margin-top: -11px"><i>IEEE Transactions on Evolutionary Computation (<strong> TEC </strong>), 25:986-1000, 2021. 
		       <strong>(<span style="color:Burlywood">SCI, IF=14.3, CCF Rank-B</span>)</strong></p>
		</td>
	</tr>
	<tr></tr>
    <tr></tr>
    <tr></tr>

<!--    <tr>	
		<td width="206">
		<img src="img/similarity-DT.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td><a href="https://ieeexplore.ieee.org/document/9833533" target="_blank">Kernelized Similarity Learning and Embedding for Dynamic Texture Synthesis.</a>
			<strong>[<a href="https://github.com/shiming-chen/Similariy-DT">Code</a>]
				[<a href="http://arxiv.org/abs/1911.04254">arXiv</a>]</strong>
				[<a href="https://shiming-chen.github.io/Similarity-page/Similarit.html">Project Page</a>]</strong>      
			<br>
		<p ><strong>Shiming Chen</strong>, Peng Zhang, Guo-sen Xie, Zehong Cao, Qinmu Peng, Wei Yuan, Xinge You.</p>
	       <p style="margin-top: -11px"><i>IEEE Transactions on Systems, Man and Cybernetics: Systems (<strong> TSMCA</strong>), 53(2):824-837, 2023. <strong> 
		 (<span style="color:Burlywood">SCI, IF=11.471</span>)</strong></p>
	<tr></tr>
    <tr></tr>
    <tr></tr> -->

<tr>	
		<td width="206">
		<img src="img/2024-TIP.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td><a href="https://ieeexplore.ieee.org/document/10558758" target="_blank">ECEA: Extensible Co-Existing Attention for Few-Shot Object Detection.</a>
  		 <strong>
		[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10558758">PDF</a>]
		[<a href="https://arxiv.org/pdf/2309.08196">arXiv</a>]
		 </strong>
		<br>
		<p >Zhimeng Xin, Tianxu Wu, <strong>Shiming Chen<sup>#</sup></strong>, Yixiong Zou, Ling Shao, Xinge You.</p>
	 <p style="margin-top: -11px"> <i>IEEE Transactions on Image Processing (<strong> TIP </strong>), 33:5564-5576, 2024. <strong>
		 (<span style="color:Burlywood">SCI, IF=10.6, CCF Rank-A</span>)</strong></p>
		</td>
	</tr>
	<tr></tr>
<tr></tr>

 <tr>	
		<td width="206">
		<img src="img/TNNLS-2024.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td><a href="https://ieeexplore.ieee.org/document/10777919" target="_blank">Visual-Semantic Graph Matching Net for Zero-Shot Learning.</a>
			<strong>
				[<a href="https://ieeexplore.ieee.org/document/10777919">PDF</a>]
				[<a href="https://arxiv.org/pdf/2411.11351">arXiv</a>]
			</strong> 
		<p >Bowen Duan, <strong>Shiming Chen<sup>#</sup></strong>, Yufei Guo, Guo-Sen Xie, Weiping Ding and Yisong Wang.</p>
	       <p style="margin-top: -11px"><i>IEEE Transactions on Neural Networks and Learning Systems (<strong> TNNLS</strong>), In press, 2024. <strong>
		 (<span style="color:Burlywood">SCI, IF=14.255, CCF Rank-B</span>)</strong></p>
	<tr></tr>
    <tr></tr>
    <tr></tr>

<tr>	
		<td width="206">
		<img src="img/2024-SCIS.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td><a href="https://ieeexplore.ieee.org/document/9833533" target="_blank">Rethinking attribute localization for zero-shot learning.</a>
			<strong>[<a href="https://link.springer.com/article/10.1007/s11432-023-4051-9">PDF</a>]</strong>
			</strong>    
			<br>
		<p >Shuhuang Chen, <strong>Shiming Chen<sup>#</sup></strong>, Guo-sen Xie, Xiangbo Shu, Xinge You, Xuelong Li.</p>
	       <p style="margin-top: -11px"><i>SCIENCE CHINA Information Sciences, 67, 172103 (2024). <strong> 
		 (<span style="color:Burlywood">SCI, IF=8.8, CCF Rank-A</span>)</strong></p>
	<tr></tr>
    <tr></tr>
    <tr></tr>
			
	
<!--     <tr>
	
		<td width="206">
		<img src="img/INS19.jpg" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="https://www.sciencedirect.com/science/article/pii/S0020025519300283?via%3Dihub" target="_blank"> Semi-Supervised Feature Learning for Improving Writer Identification.</a>
			<strong>[<a href="https://github.com/shiming-chen/Writer-Identification-WLSR">Code</a>]</strong><br>
		 <p ><strong>Shiming Chen</strong>, Yisong Wang, Chin-Teng Lin, Weiping Ding, Zehong Cao.</p>
        <p class="post-date" style="margin-top: -10px" ><i>Information Sciences (<strong> INS </strong>)</i>, 482:156-170, 2019. 
		<strong>(<span style="color:Burlywood">SCI, IF=8.233</span>)</strong></p>
		</td>
	</tr>
	<tr></tr>
    <tr></tr>
    <tr></tr>

    <tr>
		<td width="206">
		<img src="img/ACTA18.jpg" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td><a href="http://www.cnki.net/kcms/doi/10.16383/j.aas.c180441.html" target="_blank"> A Robust Offline Writer Identification Method.</a>
			<strong>[<a href="https://github.com/shiming-chen/DLS-CNN">Code</a>]</strong><br>
		<p><strong>Shiming Chen</strong>, Yisong Wang.</p>
        <p class="post-date" style="margin-top: -10px" ><i>ACTA AUTOMATICA SINICA (自动化学报)</i>, 46(1):108-116, 2020.  
		<strong>(<span style="color:Burlywood">In Chinese, CAA-A,CCF-A,卓越期刊</span>)</strong></p>
		</td>
	</tr>
	<tr></tr>
    <tr></tr>
    <tr></tr> -->

</tbody>
</table>
	  
	  
    <h2><strong>Awards</strong></h2>
   <div>
	<li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2024.11</span>, <strong>2024 CSIG Excellent Doctoral Dissertation Award Honorable Mention Award</strong>.</li>
	<li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2022.08</span>, <strong>Huawei Academic Star</strong>.</li>
	<li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2022.10</span>, <strong> China National Scholarship</strong>.</li>
    </div>  
   
    <h2>Work Experience</h2>
	<ul>
	<li>
		<div style="float:left; text-align:left">Trustworthy Machine Learning Lab (TML Lab), Uinversity of Sydney, Working with Prof. <a href="https://tongliang-liu.github.io/index.html">Tongliang Liu</a></div> <div style="float:right; text-align:right">April. 2022 – Dec. 2022</div><br>
		Visiting Student<br>
		Topic: Zero-shot Learning, Noisy Label<br>
	</li>
	<li>
		<div style="float:left; text-align:left">Tencent, AI Lab. </div> <div style="float:right; text-align:right">Jan. 2022 – Dec. 2022</div><br>
		Research Intern<br>
		Topic: Zero-shot Learning, Visual-and-Language System<br>
	</li>
        <li>
		<div style="float:left; text-align:left">Alibaba DAMO Academy, Machine Intelligence, Vision Lab. Hosted by <a href="https://www.cse.msu.edu/~rongjin/">Rong Jin</a> </div> <div style="float:right; text-align:right">May. 2021 – Oct. 2021</div><br>
		Research Intern<br>
		Topic: Zero-shot Learning, Domain Adaptation, Transformer<br>
	</li>
	<li>
		<div style="float:left; text-align:left">Fortsense, In-Screen Fingerprint Group. </div> <div style="float:right; text-align:right">Apr. 2020 – Sep. 2020</div><br>
		Research Intern<br>
		Topic: Image Retrieval, Signal Verification<br>
	</li>
	
	</ul>  
	 
	 
     <h2><strong>Professional Services</strong></h2>
   <div>
	<strong>Area Chair:</strong> 
	PRCV'23, VALSE.<br>
	   
	<strong>Journal Reviewers:</strong> 
	TPAMI, IJCV, TIP, TNNLS, TEC, TCYB, TSMCA, TITS, TII, TMM, TASE, TIV, etc.<br>
	
	<strong>Conference PC/Reviewers:</strong> 
	ICLR'23-24, NeurIPS'23, CVPR'22-24, ICCV'21-23, ECCV'22-24, AAAI'22-24,  IJCAI'21-23, ACM MM'21.
    </div>
	
	 
     <h2>Invited Talks</h2>
     <div style="height: 320px; overflow: auto;">
      <ul>
	<li>
		<div style="float:left; text-align:left; color:＃BDB76B"><strong>School of Information Science and Technology, University of Science and Technology of China </strong></div> <div style="float:right; text-align:right">Jun. 2023</div><br>
		<strong>Title</strong>: Zero-Shot Learning in Vision<br>
	</li>
	<li>
		<div style="float:left; text-align:left; color:＃BDB76B"><strong>Alibaba DAMO Academic </strong></div> <div style="float:right; text-align:right">Apr. 2023</div><br>
		<strong>Title</strong>: Semantic-Guided Zero-Shot Learning<br>
	</li>
	<li>
		<div style="float:left; text-align:left; color:＃BDB76B"><strong>Department of Mathematics and Statistics, Huazhong Agricultural Univeristy</strong></div> <div style="float:right; text-align:right">Apr. 2023</div><br>
		<strong>Title</strong>: Attribute Based Zero-Shot Learning<br>
	</li>
	<li>
		<div style="float:left; text-align:left; color:＃BDB76B"><strong>School of Computer Science and Technology, Guizhou University</strong></div> <div style="float:right; text-align:right">Apr. 2023</div><br>
		<strong>Title</strong>: Semantic-Guided Zero-Shot Learning<br>
	</li>
	
	<li>
		<div style="float:left; text-align:left; color:＃BDB76B"><strong>National Key Laboratory of Science and Technology on Multispectral Information Processing</strong></div> <div style="float:right; text-align:right">Mar. 2023</div><br>
		<strong>Title</strong>: Deep Feature Representations Based Zero-Shot Learning<br>
	</li>
	<li>
		<div style="float:left; text-align:left; color:＃BDB76B"><strong>IEEE International Conference on Digital Twins and Parallel Intelligence</strong></div> <div style="float:right; text-align:right">Nov. 2022</div><br>
		<strong>Title</strong>: Mutually Semantic Distillation Network for Zero-Shot Learning<br>
	</li>
	<li>
		<div style="float:left; text-align:left; color:＃BDB76B"><strong>Huawei (Shanghai)</strong></div> <div style="float:right; text-align:right">Aug. 2022</div><br>
		<strong>Title</strong>: Deep Feature Representations Based Zero-Shot Image Classification<br>
	</li>
	<li>
		<div style="float:left; text-align:left; color:＃BDB76B"><strong>Zhidongxi (AI New Youth)</strong></div> <div style="float:right; text-align:right">Aug. 2022</div><br>
		<strong>Title</strong>: MSDN: Mutually Semantic Distillation Network for Zero-Shot Learning<br>
<!-- 		<strong>Link</strong>: 
		[<a href="https://apposcmf8kb5033.h5.xiaoeknow.com/v2/course/alive/l_62f36b2ee4b0eca59c2127c7?type=2&app_id=appoSCMf8kb5033&pro_id=p_6214b182e4b066e96087ec57&available=true&share_user_id=u_628c93c5dc641_P1UyQhrGmR&share_type=5&scene=%E5%88%86%E4%BA%AB&share_scene=1&entry=2&entry_type=2002&state=3e66fce1cd6f09304d0691d2f4f91ef9_tDzhxX">Video</a>] -->
	</li>
	<li>
		<div style="float:left; text-align:left; color:＃BDB76B"><strong>VALSE</strong></div> <div style="float:right; text-align:right">Jun. 2022</div><br>
		<strong>Title</strong>: Mutually Semantic Distillation Network for Zero-Shot Learning<br>
<!-- 		<strong>Link</strong>: [<a href="https://mp.weixin.qq.com/s/EpzaYMd49QZk8qE0Q271iA">VALSE</a>]
		[<a href="https://www.bilibili.com/video/BV1ag411o7kz?spm_id_from=333.337.search-card.all.click">Video</a>] -->
	</li>
 	<li>
		<div style="float:left; text-align:left; color:＃BDB76B"><strong>AI Drive</strong></div> <div style="float:right; text-align:right">Jun. 2022</div><br>
		<strong>Title</strong>: Mutually Semantic Distillation Network for Zero-Shot Learning<br>
<!-- 		<strong>Link</strong>: [<a href="https://www.bilibili.com/video/BV1ag411o7kz?spm_id_from=333.337.search-card.all.click">Video</a>] -->
	</li>
	<li>
		<div style="float:left; text-align:left; color:＃BDB76B"><strong>AI TIME</strong></div> <div style="float:right; text-align:right">Jun. 2022</div><br>
		<strong>Title</strong>: Attribute-guided Transformer for Zero-Shot Learning<br>
<!-- 		<strong>Link</strong>: [<a href="https://www.bilibili.com/video/BV1kR4y1w7yj">Video</a>] -->
	</li>
	<li>
		<div style="float:left; text-align:left; color:＃BDB76B"><strong>Extreme Mart</strong></div> <div style="float:right; text-align:right">Feb. 2022</div><br>
		<strong>Title</strong>: Research on Key Technology for Zero-shot Learning<br>
<!-- 		<strong>Link</strong>: [<a href="https://www.bilibili.com/video/BV14b4y1W7dp/">Video</a>] -->
	</li>
       <li>
		<div style="float:left; text-align:left; color:＃BDB76B"><strong>AI TIME</strong></div> <div style="float:right; text-align:right">Feb. 2022</div><br>
		<strong>Title</strong>: 基于层次适应的零样本学习<br>
<!-- 	       <strong>Link</strong>: [<a href="https://www.bilibili.com/video/BV1X44y1H7S4">Video</a>] -->
	</li>
        <li>
		<div style="float:left; text-align:left; color:＃BDB76B"><strong>CVTE</strong></div> <div style="float:right; text-align:right">Dec. 2021</div><br>
		<strong>Title</strong>: Recent Advances in Zero-Shot Learning<br>
	</li>
	<li>
		<div style="float:left; text-align:left; color:＃BDB76B"><strong>Tencent AI Lab </strong></div> <div style="float:right; text-align:right">Dec. 2021</div><br>
		<strong>Title</strong>: The Frontiers in Zero-Shot Learning<br>
	</li>
		
	<li>
		<div style="float:left; text-align:left; color:＃BDB76B"><strong>Alibaba DAMO Academic </strong></div> <div style="float:right; text-align:right">Aug. 2021</div><br>
		<strong>Title</strong>: Hierarchical Semantic-Visual Adaptation for Zero-Shot Learning<br>
	</li>
	
	</ul>
        </div>

     
	
	  
	  
	  

     <div id="footer">
	<div id="footer-text"></div>
    </div>
    <div id = "logo" style="margin-top: 10px; text-align:center">
	    <div align="center" style="margin:auto;padding-top:10px">
            <div style="width:12%">
                <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=kKMhkSHMeApLgHGqTfk5xOrHScwhlrHiRwRbeYjigWg"></script>
            </div>
           <br>
        &copy; Shiming Chen | <span style="color:Red">Last updated: Sept. 20, 2023</span>
            </div>
     </div>     
	
   <!-- <div id = "logo" style="margin-top: 10px; text-align:center">
    <a href="http://www.hust.edu.cn/"><img  src="img/HUST.jpg" height="65px"  /></a> 
     <a href="http://www.gzu.edu.cn/"><img  src="img/GZU.jpg" height="65px" style="margin-left: 50px"/></a> -->
    

    
  </div>

</body></html>
